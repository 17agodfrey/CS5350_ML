{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BOW_train = pd.read_csv('../project_data/data/bag-of-words/bow.train.csv').replace({'label': {0: -1}})\n",
    "BOW_test = pd.read_csv('../project_data/data/bag-of-words/bow.test.csv').replace({'label': {0: -1}})\n",
    "BOW_eval = pd.read_csv('../project_data/data/bag-of-words/bow.eval.anon.csv').replace({'label': {0: -1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_train = pd.read_csv('../project_data/data/glove/glove.train.csv').replace({'label': {0: -1}})\n",
    "glove_test = pd.read_csv('../project_data/data/glove/glove.test.csv').replace({'label': {0: -1}})\n",
    "glove_eval = pd.read_csv('../project_data/data/glove/glove.eval.anon.csv').replace({'label': {0: -1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = pd.read_csv('../project_data/data/tfidf/tfidf.train.csv').replace({'label': {0: -1}})    \n",
    "tfidf_test = pd.read_csv('../project_data/data/tfidf/tfidf.test.csv').replace({'label': {0: -1}})\n",
    "tfidf_eval = pd.read_csv('../project_data/data/tfidf/tfidf.eval.anon.csv').replace({'label': {0: -1}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_sub_gradient_descent_SVM(train_data, train_labels, LR_0, C, T, SVM_obj_threshold) :\n",
    "    N = len(train_data)\n",
    "    w = np.zeros(len(train_data[0]))\n",
    "    LR_t = LR_0\n",
    "    prev_SVM_obj = None\n",
    "    for t in range(T):\n",
    "        # shuffle the data\n",
    "        indices = list(range(N))\n",
    "        random.shuffle(indices) ### this may not work, \n",
    "        # calculate the learning rate\n",
    "        LR_t = LR_0/(1+t)\n",
    "        for i in range(N):\n",
    "            yi = train_labels[i]\n",
    "            xi = train_data[i]\n",
    "            if yi * np.dot(w, xi) <= 1:\n",
    "                w = (1-LR_t)*w + LR_t*C*(yi * xi)\n",
    "            else:\n",
    "                w = (1-LR_t)*w\n",
    "        # calculate SVM objective after each epoch, may have to sum but idk\n",
    "        SVM_obj = np.sum(np.maximum(0, 1 - train_labels * np.dot(train_data, w))) + 0.5 * C * np.dot(w, w)\n",
    "        # may not even need this part\n",
    "        if prev_SVM_obj == None:\n",
    "            prev_SVM_obj = SVM_obj\n",
    "            continue\n",
    "        # stop iterating if change in SVM objective is smaller than threshold\n",
    "        if prev_SVM_obj - SVM_obj < SVM_obj_threshold:\n",
    "            break\n",
    "        prev_SVM_obj = SVM_obj\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modifying this from assignment to use accuracy instead of F_1\n",
    "\n",
    "def CV (train_folds, C, LR_0, T, SVM_obj_threshold):\n",
    "    accuracy = 0\n",
    "    for i in range(5):\n",
    "        test_data = train_folds[i].drop('label', axis=1).values\n",
    "        test_labels = train_folds[i]['label'].values\n",
    "        train_data = pd.concat([train_folds[j] for j in range(5) if j != i]).drop('label', axis=1).values\n",
    "        train_labels = pd.concat([train_folds[j] for j in range(5) if j != i])['label'].values\n",
    "        w = stochastic_sub_gradient_descent_SVM(train_data, train_labels, LR_0, C, T, SVM_obj_threshold)\n",
    "        N = len(test_data)\n",
    "        correct = 0\n",
    "        for i in range(N):\n",
    "            # this is where we need to do the false/true positive/negative stuff\n",
    "            prediction = np.dot(w, test_data[i])\n",
    "            actual = test_labels[i] \n",
    "            if prediction > 0 and actual == 1:\n",
    "                correct += 1\n",
    "            elif prediction < 0 and actual == -1:\n",
    "                correct += 1\n",
    "        accuracy += correct/N\n",
    "            \n",
    "    return accuracy/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_guesses_to_csv(w, data, filename):\n",
    "    examples = []\n",
    "    guesses = []\n",
    "    i = 0\n",
    "    # majority_label = dtID3.determine_majority_label(data)\n",
    "    for row in data.iterrows():\n",
    "        examples.append(i)\n",
    "        # print(row[1].shape)\n",
    "        # print(a.shape)\n",
    "        # print(a)\n",
    "        guessLabel = np.dot(w, row[1].values)\n",
    "        if guessLabel > 0:\n",
    "            guesses.append(1)\n",
    "        else:\n",
    "            guesses.append(0)\n",
    "        i += 1\n",
    "    examples_df = pd.DataFrame(examples, columns=['example_id'])\n",
    "    guesses_df = pd.DataFrame(guesses, columns=['label'])\n",
    "    guesses_df = pd.concat([examples_df, guesses_df], axis=1)\n",
    "    guesses_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- GLOVE ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_train_smol = glove_train.iloc[:100]\n",
    "glove_train_smol_folds = np.array_split(glove_train_smol, 5)\n",
    "glove_train_folds = np.array_split(glove_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_0 = 1, C = 10, avg_accuracy = 0.49531428571428576\n",
      "LR_0 = 1, C = 1, avg_accuracy = 0.49531428571428576\n",
      "LR_0 = 1, C = 0.1, avg_accuracy = 0.49531428571428576\n",
      "LR_0 = 1, C = 0.01, avg_accuracy = 0.49531428571428576\n",
      "LR_0 = 1, C = 0.001, avg_accuracy = 0.4953714285714286\n",
      "LR_0 = 1, C = 0.0001, avg_accuracy = 0.4953714285714286\n",
      "LR_0 = 0.1, C = 10, avg_accuracy = 0.5063428571428571\n",
      "LR_0 = 0.1, C = 1, avg_accuracy = 0.5065714285714286\n",
      "LR_0 = 0.1, C = 0.1, avg_accuracy = 0.5076\n",
      "LR_0 = 0.1, C = 0.01, avg_accuracy = 0.4958857142857143\n",
      "LR_0 = 0.1, C = 0.001, avg_accuracy = 0.5060571428571429\n",
      "LR_0 = 0.1, C = 0.0001, avg_accuracy = 0.502\n",
      "LR_0 = 0.01, C = 10, avg_accuracy = 0.5057142857142858\n",
      "LR_0 = 0.01, C = 1, avg_accuracy = 0.5069714285714286\n",
      "LR_0 = 0.01, C = 0.1, avg_accuracy = 0.5327999999999999\n",
      "LR_0 = 0.01, C = 0.01, avg_accuracy = 0.5660571428571428\n",
      "LR_0 = 0.01, C = 0.001, avg_accuracy = 0.5098857142857143\n",
      "LR_0 = 0.01, C = 0.0001, avg_accuracy = 0.5157714285714287\n",
      "LR_0 = 0.001, C = 10, avg_accuracy = 0.5316\n",
      "LR_0 = 0.001, C = 1, avg_accuracy = 0.5777714285714286\n",
      "LR_0 = 0.001, C = 0.1, avg_accuracy = 0.6059428571428571\n",
      "LR_0 = 0.001, C = 0.01, avg_accuracy = 0.5618857142857143\n",
      "LR_0 = 0.001, C = 0.001, avg_accuracy = 0.5452571428571428\n",
      "LR_0 = 0.001, C = 0.0001, avg_accuracy = 0.5429714285714285\n",
      "LR_0 = 0.0001, C = 10, avg_accuracy = 0.6112571428571428\n",
      "LR_0 = 0.0001, C = 1, avg_accuracy = 0.639142857142857\n",
      "LR_0 = 0.0001, C = 0.1, avg_accuracy = 0.6195428571428572\n",
      "LR_0 = 0.0001, C = 0.01, avg_accuracy = 0.5602285714285713\n",
      "LR_0 = 0.0001, C = 0.001, avg_accuracy = 0.5196571428571428\n",
      "LR_0 = 0.0001, C = 0.0001, avg_accuracy = 0.5198285714285714\n",
      "best Hyperparameters:  {'LR_0': 0.0001, 'C': 1, 'avg_accuracy': 0.639142857142857}\n"
     ]
    }
   ],
   "source": [
    "### testing: detemine best hyperparameters \n",
    "\n",
    "T = 100 # we probs (shouldn't) reach this, cause it should stop on its own cause we'll converge to the bottom of the bowl\n",
    "SVM_obj_threshold = 0.001\n",
    "LR_0_arr = [1, .1, .01, .001, .0001]\n",
    "C_arr = [10, 1, .1, .01, .001, .0001]\n",
    "\n",
    "bestHyperparameters = {\"LR_0\": 0, \"C\": 0, \"avg_accuracy\": 0}\n",
    "\n",
    "train_folds = glove_train_folds\n",
    "for LR_0 in LR_0_arr:\n",
    "    for C in C_arr:\n",
    "        avg_accuracy = CV(train_folds, C, LR_0, T, SVM_obj_threshold)\n",
    "        print(f'LR_0 = {LR_0}, C = {C}, avg_accuracy = {avg_accuracy}')\n",
    "        if avg_accuracy > bestHyperparameters[\"avg_accuracy\"]:\n",
    "            bestHyperparameters[\"LR_0\"] = LR_0\n",
    "            bestHyperparameters[\"C\"] = C\n",
    "            bestHyperparameters[\"avg_accuracy\"] = avg_accuracy\n",
    "print(\"best Hyperparameters: \", bestHyperparameters)       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Test Set: -------------------\n",
      "Accuracy:  0.6484444444444445\n"
     ]
    }
   ],
   "source": [
    "### testing: using best hyperparameters, train model and test on test data\n",
    "\n",
    "#####\n",
    "train = glove_train\n",
    "test = glove_test\n",
    "#####\n",
    "\n",
    "LR_0 = bestHyperparameters[\"LR_0\"]\n",
    "C = bestHyperparameters[\"C\"]\n",
    "\n",
    "train_data = train.drop('label', axis=1).values\n",
    "train_labels = train['label'].values\n",
    "\n",
    "w = stochastic_sub_gradient_descent_SVM(train_data, train_labels, LR_0, C, T, SVM_obj_threshold)\n",
    "\n",
    "test_data = test.drop('label', axis=1).values\n",
    "test_labels = test['label'].values\n",
    "N = len(test_data)\n",
    "correct = 0\n",
    "accuracy = 0\n",
    "for i in range(N):\n",
    "    # this is where we need to do the false/true positive/negative stuff\n",
    "    prediction = np.dot(w, test_data[i])\n",
    "    actual = test_labels[i] \n",
    "    if prediction > 0 and actual == 1:\n",
    "        correct += 1\n",
    "    elif prediction < 0 and actual == -1:\n",
    "        correct += 1\n",
    "accuracy = correct/N\n",
    "\n",
    "\n",
    "print(\"On Test Set: -------------------\")\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_guesses_to_csv(w, glove_eval.drop(columns=['label']), \"SVM.glove.eval.predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------- tfidf -------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_folds = np.array_split(tfidf_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_0 = 1, C = 10, avg_accuracy = 0.5493714285714285\n",
      "LR_0 = 1, C = 1, avg_accuracy = 0.5091428571428571\n",
      "LR_0 = 1, C = 0.1, avg_accuracy = 0.5091428571428571\n",
      "LR_0 = 1, C = 0.01, avg_accuracy = 0.5091428571428571\n",
      "LR_0 = 1, C = 0.001, avg_accuracy = 0.5091428571428571\n",
      "LR_0 = 1, C = 0.0001, avg_accuracy = 0.5091428571428571\n",
      "LR_0 = 0.1, C = 10, avg_accuracy = 0.5792\n",
      "LR_0 = 0.1, C = 1, avg_accuracy = 0.5795428571428572\n",
      "LR_0 = 0.1, C = 0.1, avg_accuracy = 0.5795428571428572\n",
      "LR_0 = 0.1, C = 0.01, avg_accuracy = 0.5795428571428572\n",
      "LR_0 = 0.1, C = 0.001, avg_accuracy = 0.5795428571428572\n",
      "LR_0 = 0.1, C = 0.0001, avg_accuracy = 0.5790285714285714\n",
      "LR_0 = 0.01, C = 10, avg_accuracy = 0.6344\n",
      "LR_0 = 0.01, C = 1, avg_accuracy = 0.6344\n",
      "LR_0 = 0.01, C = 0.1, avg_accuracy = 0.6344\n",
      "LR_0 = 0.01, C = 0.01, avg_accuracy = 0.6344\n",
      "LR_0 = 0.01, C = 0.001, avg_accuracy = 0.6344\n",
      "LR_0 = 0.01, C = 0.0001, avg_accuracy = 0.6344\n",
      "LR_0 = 0.001, C = 10, avg_accuracy = 0.6809714285714286\n",
      "LR_0 = 0.001, C = 1, avg_accuracy = 0.6813142857142858\n",
      "LR_0 = 0.001, C = 0.1, avg_accuracy = 0.6814285714285714\n",
      "LR_0 = 0.001, C = 0.01, avg_accuracy = 0.6793714285714285\n",
      "LR_0 = 0.001, C = 0.001, avg_accuracy = 0.6799428571428572\n",
      "LR_0 = 0.001, C = 0.0001, avg_accuracy = 0.6799428571428572\n",
      "LR_0 = 0.0001, C = 10, avg_accuracy = 0.6805142857142857\n",
      "LR_0 = 0.0001, C = 1, avg_accuracy = 0.6801714285714286\n",
      "LR_0 = 0.0001, C = 0.1, avg_accuracy = 0.6809714285714286\n",
      "LR_0 = 0.0001, C = 0.01, avg_accuracy = 0.6813142857142858\n",
      "LR_0 = 0.0001, C = 0.001, avg_accuracy = 0.6813714285714286\n",
      "LR_0 = 0.0001, C = 0.0001, avg_accuracy = 0.6816000000000001\n",
      "best Hyperparameters:  {'LR_0': 0.0001, 'C': 0.0001, 'avg_accuracy': 0.6816000000000001}\n"
     ]
    }
   ],
   "source": [
    "### testing: detemine best hyperparameters \n",
    "\n",
    "T = 100 # we probs (shouldn't) reach this, cause it should stop on its own cause we'll converge to the bottom of the bowl\n",
    "SVM_obj_threshold = 0.001\n",
    "LR_0_arr = [1, .1, .01, .001, .0001]\n",
    "C_arr = [10, 1, .1, .01, .001, .0001]\n",
    "\n",
    "bestHyperparameters = {\"LR_0\": 0, \"C\": 0, \"avg_accuracy\": 0}\n",
    "\n",
    "train_folds = tfidf_train_folds\n",
    "\n",
    "for LR_0 in LR_0_arr:\n",
    "    for C in C_arr:\n",
    "        avg_accuracy = CV(train_folds, C, LR_0, T, SVM_obj_threshold)\n",
    "        print(f'LR_0 = {LR_0}, C = {C}, avg_accuracy = {avg_accuracy}')\n",
    "        if avg_accuracy > bestHyperparameters[\"avg_accuracy\"]:\n",
    "            bestHyperparameters[\"LR_0\"] = LR_0\n",
    "            bestHyperparameters[\"C\"] = C\n",
    "            bestHyperparameters[\"avg_accuracy\"] = avg_accuracy\n",
    "print(\"best Hyperparameters: \", bestHyperparameters)       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Test Set: -------------------\n",
      "Accuracy:  0.688\n"
     ]
    }
   ],
   "source": [
    "### testing: using best hyperparameters, train model and test on test data\n",
    "\n",
    "#####\n",
    "train = tfidf_train\n",
    "test = tfidf_test\n",
    "#####\n",
    "\n",
    "LR_0 = bestHyperparameters[\"LR_0\"]\n",
    "C = bestHyperparameters[\"C\"]\n",
    "\n",
    "train_data = train.drop('label', axis=1).values\n",
    "train_labels = train['label'].values\n",
    "\n",
    "w = stochastic_sub_gradient_descent_SVM(train_data, train_labels, LR_0, C, T, SVM_obj_threshold)\n",
    "\n",
    "test_data = test.drop('label', axis=1).values\n",
    "test_labels = test['label'].values\n",
    "N = len(test_data)\n",
    "correct = 0\n",
    "accuracy = 0\n",
    "for i in range(N):\n",
    "    # this is where we need to do the false/true positive/negative stuff\n",
    "    prediction = np.dot(w, test_data[i])\n",
    "    actual = test_labels[i] \n",
    "    if prediction > 0 and actual == 1:\n",
    "        correct += 1\n",
    "    elif prediction < 0 and actual == -1:\n",
    "        correct += 1\n",
    "accuracy = correct/N\n",
    "\n",
    "\n",
    "print(\"On Test Set: -------------------\")\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_guesses_to_csv(w, tfidf_eval.drop(columns=['label']), \"SVM.tfidf.eval.predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- BOW ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_train_folds = np.array_split(BOW_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_0 = 1, C = 10, avg_accuracy = 0.5051428571428571\n",
      "LR_0 = 1, C = 1, avg_accuracy = 0.5089714285714285\n",
      "LR_0 = 1, C = 0.1, avg_accuracy = 0.5047428571428572\n",
      "LR_0 = 1, C = 0.01, avg_accuracy = 0.5030857142857144\n",
      "LR_0 = 1, C = 0.001, avg_accuracy = 0.5030285714285715\n",
      "LR_0 = 1, C = 0.0001, avg_accuracy = 0.5031428571428572\n",
      "LR_0 = 0.1, C = 10, avg_accuracy = 0.5584\n",
      "LR_0 = 0.1, C = 1, avg_accuracy = 0.5409142857142857\n",
      "LR_0 = 0.1, C = 0.1, avg_accuracy = 0.5689714285714286\n",
      "LR_0 = 0.1, C = 0.01, avg_accuracy = 0.5445142857142857\n",
      "LR_0 = 0.1, C = 0.001, avg_accuracy = 0.5125714285714287\n",
      "LR_0 = 0.1, C = 0.0001, avg_accuracy = 0.5125714285714287\n",
      "LR_0 = 0.01, C = 10, avg_accuracy = 0.6089142857142857\n",
      "LR_0 = 0.01, C = 1, avg_accuracy = 0.6270857142857142\n",
      "LR_0 = 0.01, C = 0.1, avg_accuracy = 0.5703428571428572\n",
      "LR_0 = 0.01, C = 0.01, avg_accuracy = 0.5562285714285714\n",
      "LR_0 = 0.01, C = 0.001, avg_accuracy = 0.5226285714285714\n",
      "LR_0 = 0.01, C = 0.0001, avg_accuracy = 0.5226285714285714\n",
      "LR_0 = 0.001, C = 10, avg_accuracy = 0.6681142857142858\n",
      "LR_0 = 0.001, C = 1, avg_accuracy = 0.6641714285714286\n",
      "LR_0 = 0.001, C = 0.1, avg_accuracy = 0.6452571428571428\n",
      "LR_0 = 0.001, C = 0.01, avg_accuracy = 0.5865142857142857\n",
      "LR_0 = 0.001, C = 0.001, avg_accuracy = 0.5649142857142857\n",
      "LR_0 = 0.001, C = 0.0001, avg_accuracy = 0.5649142857142857\n",
      "LR_0 = 0.0001, C = 10, avg_accuracy = 0.6909714285714286\n",
      "LR_0 = 0.0001, C = 1, avg_accuracy = 0.6644571428571429\n",
      "LR_0 = 0.0001, C = 0.1, avg_accuracy = 0.6582285714285714\n",
      "LR_0 = 0.0001, C = 0.01, avg_accuracy = 0.5950285714285714\n",
      "LR_0 = 0.0001, C = 0.001, avg_accuracy = 0.584342857142857\n",
      "LR_0 = 0.0001, C = 0.0001, avg_accuracy = 0.5833714285714285\n",
      "best Hyperparameters:  {'LR_0': 0.0001, 'C': 10, 'avg_accuracy': 0.6909714285714286}\n"
     ]
    }
   ],
   "source": [
    "### testing: detemine best hyperparameters \n",
    "\n",
    "T = 100 # we probs (shouldn't) reach this, cause it should stop on its own cause we'll converge to the bottom of the bowl\n",
    "SVM_obj_threshold = 0.001\n",
    "LR_0_arr = [1, .1, .01, .001, .0001]\n",
    "C_arr = [10, 1, .1, .01, .001, .0001]\n",
    "\n",
    "bestHyperparameters = {\"LR_0\": 0, \"C\": 0, \"avg_accuracy\": 0}\n",
    "\n",
    "train_folds = BOW_train_folds\n",
    "\n",
    "for LR_0 in LR_0_arr:\n",
    "    for C in C_arr:\n",
    "        avg_accuracy = CV(train_folds, C, LR_0, T, SVM_obj_threshold)\n",
    "        print(f'LR_0 = {LR_0}, C = {C}, avg_accuracy = {avg_accuracy}')\n",
    "        if avg_accuracy > bestHyperparameters[\"avg_accuracy\"]:\n",
    "            bestHyperparameters[\"LR_0\"] = LR_0\n",
    "            bestHyperparameters[\"C\"] = C\n",
    "            bestHyperparameters[\"avg_accuracy\"] = avg_accuracy\n",
    "print(\"best Hyperparameters: \", bestHyperparameters)       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Test Set: -------------------\n",
      "Accuracy:  0.7075555555555556\n"
     ]
    }
   ],
   "source": [
    "### testing: using best hyperparameters, train model and test on test data\n",
    "\n",
    "#####\n",
    "train = BOW_train\n",
    "test = BOW_test\n",
    "#####\n",
    "\n",
    "LR_0 = bestHyperparameters[\"LR_0\"]\n",
    "C = bestHyperparameters[\"C\"]\n",
    "\n",
    "train_data = train.drop('label', axis=1).values\n",
    "train_labels = train['label'].values\n",
    "\n",
    "w = stochastic_sub_gradient_descent_SVM(train_data, train_labels, LR_0, C, T, SVM_obj_threshold)\n",
    "\n",
    "test_data = test.drop('label', axis=1).values\n",
    "test_labels = test['label'].values\n",
    "N = len(test_data)\n",
    "correct = 0\n",
    "accuracy = 0\n",
    "for i in range(N):\n",
    "    # this is where we need to do the false/true positive/negative stuff\n",
    "    prediction = np.dot(w, test_data[i])\n",
    "    actual = test_labels[i] \n",
    "    if prediction > 0 and actual == 1:\n",
    "        correct += 1\n",
    "    elif prediction < 0 and actual == -1:\n",
    "        correct += 1\n",
    "accuracy = correct/N\n",
    "\n",
    "\n",
    "print(\"On Test Set: -------------------\")\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_guesses_to_csv(w, BOW_eval.drop(columns=['label']), \"SVM.BOW.eval.predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
